{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "adaabc78",
      "metadata": {
        "id": "adaabc78",
        "outputId": "4e2ee202-df1a-4bee-9fbc-1f92e7ec76d2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x1ac5bb04630>"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import TensorDataset\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.express as px\n",
        "import plotly.graph_objs as go\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from fastprogress import progress_bar\n",
        "\n",
        "import os\n",
        "\n",
        "torch.manual_seed(0) # fix seed"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c7cef8d6",
      "metadata": {
        "id": "c7cef8d6"
      },
      "source": [
        "### Parameters Set based on Model Training Results"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b0983c8d",
      "metadata": {
        "id": "b0983c8d"
      },
      "source": [
        "![Model](https://github.com/MK-ek11/stock_price_forecast_with_LSTM/blob/main/images/model_plot.png?raw=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dec23c79",
      "metadata": {
        "id": "dec23c79"
      },
      "outputs": [],
      "source": [
        "# Set Parameter\n",
        "sequence_length = 7 \n",
        "batch = 64\n",
        "hidden_size = 100\n",
        "num_epochs = 150"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "44583702",
      "metadata": {
        "id": "44583702"
      },
      "source": [
        "### Set device \n",
        "\n",
        "- use CUDA ref: <https://pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html#creating-models>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "739a152e",
      "metadata": {
        "id": "739a152e",
        "outputId": "a089e654-3af1-4fed-df20-1a51c9332cf9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using cuda device\n"
          ]
        }
      ],
      "source": [
        "device = (\n",
        "    \"cuda\"\n",
        "    if torch.cuda.is_available()\n",
        "    else \"mps\"\n",
        "    if torch.backends.mps.is_available()\n",
        "    else \"cpu\"\n",
        ")\n",
        "print(f\"Using {device} device\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "473a2641",
      "metadata": {
        "id": "473a2641"
      },
      "source": [
        "### Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d02d49a0",
      "metadata": {
        "id": "d02d49a0"
      },
      "outputs": [],
      "source": [
        "# Create function for getting index of a date\n",
        "def start_date_index(year, month, dataset):\n",
        "    start_date = f\"{year}-{month}\"\n",
        "    index_start_date = dataset[dataset[\"Date\"].str.contains(start_date, regex=False)].index[0]\n",
        "    \n",
        "    print(f\"Index of Date: {index_start_date}\\nStart Date: {dataset.iloc[index_start_date,0]}\", end=\"\\n\"*2)\n",
        "    return index_start_date"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "74573448",
      "metadata": {
        "id": "74573448"
      },
      "outputs": [],
      "source": [
        "# Create function for splitting dataset\n",
        "def split_dataset(start_index, sequence_length, dataset):\n",
        "    train_set = dataset.loc[0:start_index-1]\n",
        "    test_set = dataset.loc[start_index-sequence_length:len(dataset)].reset_index(drop=True)\n",
        "    return train_set, test_set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a6ee3c2b",
      "metadata": {
        "id": "a6ee3c2b"
      },
      "outputs": [],
      "source": [
        "# Create function for Training set Sliding Window\n",
        "def sliding_window_train(sequence_length, input_data):\n",
        "    train = []\n",
        "    target = []\n",
        "\n",
        "    for i in range(sequence_length, len(input_data)):\n",
        "        train.append(input_data[i-sequence_length:i])\n",
        "        target.append(input_data[i])\n",
        "\n",
        "    train = np.array(train)\n",
        "    target = np.array(target)\n",
        "    \n",
        "    print(f\"Train shape: {train.shape}\",\n",
        "         end=\"\\n\"*2)\n",
        "    print(f\"Train:\\nbatch size: {train.shape[0]}\\nsequence length: {train.shape[1]} days\\ninput size: {train.shape[2]}\",\n",
        "          end=\"\\n\"*3)\n",
        "    print(f\"Target shape: {target.shape}\",\n",
        "         end=\"\\n\"*2)    \n",
        "    print(f\"Target:\\nbatch size: {target.shape[0]}\\ninput size: {target.shape[1]}\",\n",
        "          end=\"\\n\"*2)\n",
        "\n",
        "    return train, target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80fe2ae0",
      "metadata": {
        "id": "80fe2ae0"
      },
      "outputs": [],
      "source": [
        "# Create function for Testing set Sliding Window\n",
        "def sliding_window_test(start_index, sequence_length, input_data_scale, input_data_nonscale, input_date):\n",
        "    test = []\n",
        "    actual = []\n",
        "    date = []\n",
        "    open_price= []\n",
        "\n",
        "    for i in range(start_index, len(input_data_scale)):\n",
        "        test.append(input_data_scale[i-sequence_length:i])\n",
        "        actual.append(input_data_scale[i])\n",
        "        date.append(input_date[i])\n",
        "        open_price.append(input_data_nonscale[i])\n",
        "\n",
        "    test = np.array(test)\n",
        "    actual = np.array(actual)\n",
        "    date = np.array(date)\n",
        "    open_price = np.array(open_price)\n",
        "    \n",
        "    print(f\"Test shape: {test.shape}\",\n",
        "         end=\"\\n\"*2)\n",
        "    print(f\"Test:\\nbatch size: {test.shape[0]}\\nsequence length: {test.shape[1]} days\\ninput size: {test.shape[2]}\",\n",
        "          end=\"\\n\"*3)\n",
        "    print(f\"Actual shape: {actual.shape}\",\n",
        "         end=\"\\n\"*2)    \n",
        "    print(f\"Actual:\\nbatch size: {actual.shape[0]}\\ninput size: {actual.shape[1]}\",\n",
        "          end=\"\\n\"*2)      \n",
        "\n",
        "    return test, actual, date, open_price"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ec931f6",
      "metadata": {
        "id": "0ec931f6"
      },
      "outputs": [],
      "source": [
        "# Create function for RMSE and MSE\n",
        "def MSE_RMSE_loss_np(predicted, target):\n",
        "    mse = (np.square(np.subtract(target,predicted))).mean()\n",
        "    rmse = np.sqrt(mse)\n",
        "    print(f\"MSE Loss: {mse} \\nRMSE Loss: {rmse}\",end=\"\\n\"*2)\n",
        "    return rmse, mse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a73fa398",
      "metadata": {
        "id": "a73fa398"
      },
      "outputs": [],
      "source": [
        "# Create function for R2 Coefficient of determination\n",
        "def R2_np(predicted, target):\n",
        "    SSR = np.square(np.subtract(target,predicted)).sum()\n",
        "    SST = np.square(target-target.mean()).sum()\n",
        "    R2 = (1-(SSR/SST))\n",
        "    print(f\"R2: {R2}\",end=\"\\n\"*2)\n",
        "    return R2"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a49c6216",
      "metadata": {
        "id": "a49c6216"
      },
      "source": [
        "### Extract Data "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f406bf8",
      "metadata": {
        "id": "7f406bf8"
      },
      "outputs": [],
      "source": [
        "# Extract data\n",
        "dataset_og = pd.read_csv(r\"dataset\\data.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "20b99567",
      "metadata": {
        "id": "20b99567",
        "outputId": "f98cfb90-594f-46d6-e680-f77cc2aee106"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Missing Data</th>\n",
              "      <th>Columns Types</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Date</th>\n",
              "      <td>0</td>\n",
              "      <td>object</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Open</th>\n",
              "      <td>0</td>\n",
              "      <td>float64</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>High</th>\n",
              "      <td>0</td>\n",
              "      <td>float64</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Low</th>\n",
              "      <td>0</td>\n",
              "      <td>float64</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Close</th>\n",
              "      <td>0</td>\n",
              "      <td>float64</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Adj Close</th>\n",
              "      <td>0</td>\n",
              "      <td>float64</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Volume</th>\n",
              "      <td>0</td>\n",
              "      <td>int64</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           Missing Data Columns Types\n",
              "Date                  0        object\n",
              "Open                  0       float64\n",
              "High                  0       float64\n",
              "Low                   0       float64\n",
              "Close                 0       float64\n",
              "Adj Close             0       float64\n",
              "Volume                0         int64"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Description for missing data and columns types\n",
        "missing_data = dataset_og.isnull().sum()\n",
        "dtypes_data = dataset_og.dtypes\n",
        "summary_df = pd.DataFrame(data = {\"Missing Data\":missing_data, \"Columns Types\":dtypes_data})\n",
        "summary_df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5acf32e1",
      "metadata": {
        "id": "5acf32e1"
      },
      "source": [
        "### Split Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5eecb11b",
      "metadata": {
        "id": "5eecb11b",
        "outputId": "185e396a-1701-4ff7-9294-4fd1f6275761"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Index of Date: 5787\n",
            "Start Date: 2023-01-03\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Define the date to split train and test\n",
        "year = \"2023\"\n",
        "month = \"01\"\n",
        "# sequence_length = 7 \n",
        "\n",
        "\n",
        "# Get the index of the date\n",
        "index_start_date = start_date_index(year, month, dataset_og)\n",
        "\n",
        "\n",
        "# build the train and test dataset\n",
        "traindata, testdata = split_dataset(index_start_date, sequence_length, dataset_og)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "13a854f4",
      "metadata": {
        "id": "13a854f4"
      },
      "source": [
        "# Training & Testing and Price Prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "93c89741",
      "metadata": {
        "id": "93c89741"
      },
      "source": [
        "#### Feature Scaling Training Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e696e8b",
      "metadata": {
        "id": "9e696e8b",
        "outputId": "64c50428-2153-4e24-81ef-ed6d8a3a60d4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(5787, 1)"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Min Max Scaling for features\n",
        "# Feature Scaling\n",
        "min_max = (0,1)\n",
        "feature = \"Open\"\n",
        "\n",
        "scale_open = MinMaxScaler(feature_range = min_max)\n",
        "data_open = traindata[feature].values.reshape(-1, 1)\n",
        "data_open_scaled = scale_open.fit_transform(data_open)\n",
        "data_open_scaled.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "12915f46",
      "metadata": {
        "id": "12915f46"
      },
      "source": [
        "#### Prepare Training Data with Sliding Window"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fcb03d49",
      "metadata": {
        "id": "fcb03d49",
        "outputId": "275911f6-d036-44a2-c82b-aac75b47c301"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train shape: (5780, 7, 1)\n",
            "\n",
            "Train:\n",
            "batch size: 5780\n",
            "sequence length: 7 days\n",
            "input size: 1\n",
            "\n",
            "\n",
            "Target shape: (5780, 1)\n",
            "\n",
            "Target:\n",
            "batch size: 5780\n",
            "input size: 1\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Prepare data with sliding window\n",
        "\n",
        "train, target = sliding_window_train(sequence_length, data_open_scaled)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "90dc0f17",
      "metadata": {
        "id": "90dc0f17"
      },
      "source": [
        "#### Prepare Training Data for Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "291921cd",
      "metadata": {
        "id": "291921cd",
        "outputId": "211e8ac3-b9e2-46eb-faa2-4164caeb71b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train shape is: torch.Size([5780, 7, 1])\n",
            "train label shape is: torch.Size([5780, 1])\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Set data to Tensor\n",
        "train = Variable(torch.Tensor(np.array(train).reshape(-1,sequence_length,1)))\n",
        "target = Variable(torch.Tensor(np.array(target).reshape(-1,1)))\n",
        "\n",
        "print(\"train shape is:\",train.size())\n",
        "print(\"train label shape is:\",target.size(), end=\"\\n\"*2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "009afeaf",
      "metadata": {
        "id": "009afeaf",
        "outputId": "7bdadf44-dc46-4e8f-943e-756696388f76"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Input into LSTM Tensor Shape: torch.Size([5780, 7, 1])\n",
            "\n",
            "Training Input Tensor: \n",
            "batch size: 5780\n",
            "sequence length: 7\n",
            "input size: 1\n",
            "\n",
            "\n",
            "Target Input into LSTM Tensor Shape: torch.Size([5780, 1])\n",
            "\n",
            "Target Input Tensor: \n",
            "batch size: 5780 \n",
            "input size: 1\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Breakdown of the Training Dataset\n",
        "print(f\"Training Input into LSTM Tensor Shape: {train.size()}\", end = \"\\n\"*2)\n",
        "print(f\"Training Input Tensor: \\nbatch size: {train.size(0)}\\nsequence length: {train.size(1)}\\ninput size: {train.size(2)}\",\n",
        "      end=\"\\n\"*3)\n",
        "\n",
        "print(f\"Target Input into LSTM Tensor Shape: {target.size()}\", end = \"\\n\"*2)\n",
        "print(f\"Target Input Tensor: \\nbatch size: {target.size(0)} \\ninput size: {target.size(1)}\",\n",
        "      end=\"\\n\"*3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b06a8d3",
      "metadata": {
        "id": "3b06a8d3"
      },
      "outputs": [],
      "source": [
        "# Convert dataset to batches\n",
        "# batch = 64\n",
        "\n",
        "dataloader = DataLoader(TensorDataset(train, target), shuffle=False, batch_size=batch)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d3fe35df",
      "metadata": {
        "id": "d3fe35df"
      },
      "source": [
        "### Define the LSTM Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0387e9de",
      "metadata": {
        "id": "0387e9de"
      },
      "outputs": [],
      "source": [
        "# Model\n",
        "class LSTM(nn.Module):\n",
        "    def __init__(self, num_classes, input_size, hidden_size, num_layers):\n",
        "        super(LSTM, self).__init__()\n",
        "        \n",
        "        self.num_classes = num_classes\n",
        "        self.num_layers = num_layers\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        \n",
        "        # LSTM\n",
        "        self.lstm1 = nn.LSTM(input_size=input_size, \n",
        "                            hidden_size=hidden_size,\n",
        "                            num_layers=num_layers, \n",
        "                            batch_first=True,\n",
        "                           )   \n",
        "        # Dropout Layer\n",
        "        self.dropout1 = nn.Dropout(p=0.2)        \n",
        "        \n",
        "        # Last Fully Connected\n",
        "        self.fc = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "        \n",
        "    def forward(self, x):\n",
        "        h_01 = Variable(torch.zeros( self.num_layers, \n",
        "                                   x.size(0),  # <== this is the batch size\n",
        "                                   self.hidden_size))\n",
        "        \n",
        "        c_01 = Variable(torch.zeros(self.num_layers, \n",
        "                                   x.size(0),  # <== this is the batch size\n",
        "                                   self.hidden_size))  \n",
        "        \n",
        "        # Propagate input through LSTM\n",
        "        output, (h_n1, c_n1) = self.lstm1(x, (h_01.to(device), c_01.to(device))) \n",
        "        dropout_out1 = self.dropout1(h_n1)\n",
        "        \n",
        "        h_n_flattened = dropout_out1.view(-1, self.hidden_size) # <= Flatten Tensor\n",
        "        fc_out = self.fc(h_n_flattened) # <= FC layer\n",
        "        \n",
        "        return fc_out"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ec490943",
      "metadata": {
        "id": "ec490943"
      },
      "source": [
        "#### Set Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4971b6d6",
      "metadata": {
        "id": "4971b6d6"
      },
      "outputs": [],
      "source": [
        "# Setting Parameters of LSTM\n",
        "num_classes = 1\n",
        "input_size = train.size(2)\n",
        "# hidden_size = 100\n",
        "num_layers = 1\n",
        "\n",
        "# Epochs and learning rate\n",
        "# num_epochs = 50\n",
        "learning_rate = 1e-3\n",
        "\n",
        "# Input into LSTM\n",
        "batch_size = train.size(0)\n",
        "seq_length = train.size(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c39c4fd",
      "metadata": {
        "id": "7c39c4fd",
        "outputId": "e382acac-8ef0-468e-c1a7-ae54d68da1a4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "LSTM(\n",
              "  (lstm1): LSTM(1, 100, batch_first=True)\n",
              "  (dropout1): Dropout(p=0.2, inplace=False)\n",
              "  (fc): Linear(in_features=100, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Initialize the LSTM model\n",
        "lstm_model = LSTM(num_classes, input_size, hidden_size, num_layers)\n",
        "lstm_model.to(device)  # <= Set model to CUDA device"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bd15eff7",
      "metadata": {
        "id": "bd15eff7"
      },
      "source": [
        "### Train model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "080e4fc2",
      "metadata": {
        "id": "080e4fc2",
        "outputId": "06902afa-a98b-414f-b7cc-ca0d925e22e9"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "                background: #F44336;\n",
              "            }\n",
              "        </style>\n",
              "      <progress value='150' class='' max='150' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      100.00% [150/150 00:48<00:00]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 0, Batch: 0, Loss: 0.0002938677207566798, Total Loss: 0.0002938677207566798\n",
            "Epoch: 1, Batch: 0, Loss: 0.27531397342681885, Total Loss: 0.27531397342681885\n",
            "Epoch: 2, Batch: 0, Loss: 0.3270838260650635, Total Loss: 0.3270838260650635\n",
            "Epoch: 3, Batch: 0, Loss: 0.031589023768901825, Total Loss: 0.031589023768901825\n",
            "Epoch: 4, Batch: 0, Loss: 0.042469322681427, Total Loss: 0.042469322681427\n",
            "Epoch: 5, Batch: 0, Loss: 0.0069220587611198425, Total Loss: 0.0069220587611198425\n",
            "Epoch: 6, Batch: 0, Loss: 0.01040133647620678, Total Loss: 0.01040133647620678\n",
            "Epoch: 7, Batch: 0, Loss: 0.014146313071250916, Total Loss: 0.014146313071250916\n",
            "Epoch: 8, Batch: 0, Loss: 0.01611393690109253, Total Loss: 0.01611393690109253\n",
            "Epoch: 9, Batch: 0, Loss: 0.0044406866654753685, Total Loss: 0.0044406866654753685\n",
            "Epoch: 10, Batch: 0, Loss: 0.0016583320684731007, Total Loss: 0.0016583320684731007\n",
            "Epoch: 11, Batch: 0, Loss: 0.0017648006323724985, Total Loss: 0.0017648006323724985\n",
            "Epoch: 12, Batch: 0, Loss: 0.002667770255357027, Total Loss: 0.002667770255357027\n",
            "Epoch: 13, Batch: 0, Loss: 0.00116818118840456, Total Loss: 0.00116818118840456\n",
            "Epoch: 14, Batch: 0, Loss: 0.00024806061992421746, Total Loss: 0.00024806061992421746\n",
            "Epoch: 15, Batch: 0, Loss: 0.0004161487449891865, Total Loss: 0.0004161487449891865\n",
            "Epoch: 16, Batch: 0, Loss: 0.0003153514117002487, Total Loss: 0.0003153514117002487\n",
            "Epoch: 17, Batch: 0, Loss: 5.897484516026452e-05, Total Loss: 5.897484516026452e-05\n",
            "Epoch: 18, Batch: 0, Loss: 0.0003476490965113044, Total Loss: 0.0003476490965113044\n",
            "Epoch: 19, Batch: 0, Loss: 0.0004152520268689841, Total Loss: 0.0004152520268689841\n",
            "Epoch: 20, Batch: 0, Loss: 0.000504436669871211, Total Loss: 0.000504436669871211\n",
            "Epoch: 21, Batch: 0, Loss: 0.0009625978418625891, Total Loss: 0.0009625978418625891\n",
            "Epoch: 22, Batch: 0, Loss: 0.0028005449566990137, Total Loss: 0.0028005449566990137\n",
            "Epoch: 23, Batch: 0, Loss: 0.005961857736110687, Total Loss: 0.005961857736110687\n",
            "Epoch: 24, Batch: 0, Loss: 0.0011371474247425795, Total Loss: 0.0011371474247425795\n",
            "Epoch: 25, Batch: 0, Loss: 0.002708998741582036, Total Loss: 0.002708998741582036\n",
            "Epoch: 26, Batch: 0, Loss: 0.00791209377348423, Total Loss: 0.00791209377348423\n",
            "Epoch: 27, Batch: 0, Loss: 0.00010115725308423862, Total Loss: 0.00010115725308423862\n",
            "Epoch: 28, Batch: 0, Loss: 0.0004948017885908484, Total Loss: 0.0004948017885908484\n",
            "Epoch: 29, Batch: 0, Loss: 0.005629120394587517, Total Loss: 0.005629120394587517\n",
            "Epoch: 30, Batch: 0, Loss: 0.0013432304840534925, Total Loss: 0.0013432304840534925\n",
            "Epoch: 31, Batch: 0, Loss: 0.004041674546897411, Total Loss: 0.004041674546897411\n",
            "Epoch: 32, Batch: 0, Loss: 0.0013926324900239706, Total Loss: 0.0013926324900239706\n",
            "Epoch: 33, Batch: 0, Loss: 0.010328825563192368, Total Loss: 0.010328825563192368\n",
            "Epoch: 34, Batch: 0, Loss: 0.0001040660499711521, Total Loss: 0.0001040660499711521\n",
            "Epoch: 35, Batch: 0, Loss: 0.0011257653823122382, Total Loss: 0.0011257653823122382\n",
            "Epoch: 36, Batch: 0, Loss: 0.00380444573238492, Total Loss: 0.00380444573238492\n",
            "Epoch: 37, Batch: 0, Loss: 0.0015587459784001112, Total Loss: 0.0015587459784001112\n",
            "Epoch: 38, Batch: 0, Loss: 0.005248954053968191, Total Loss: 0.005248954053968191\n",
            "Epoch: 39, Batch: 0, Loss: 0.002145430538803339, Total Loss: 0.002145430538803339\n",
            "Epoch: 40, Batch: 0, Loss: 0.0038854083977639675, Total Loss: 0.0038854083977639675\n",
            "Epoch: 41, Batch: 0, Loss: 0.00030380437965504825, Total Loss: 0.00030380437965504825\n",
            "Epoch: 42, Batch: 0, Loss: 7.690320489928126e-05, Total Loss: 7.690320489928126e-05\n",
            "Epoch: 43, Batch: 0, Loss: 0.0016233782516792417, Total Loss: 0.0016233782516792417\n",
            "Epoch: 44, Batch: 0, Loss: 0.002348464448004961, Total Loss: 0.002348464448004961\n",
            "Epoch: 45, Batch: 0, Loss: 0.0021965212654322386, Total Loss: 0.0021965212654322386\n",
            "Epoch: 46, Batch: 0, Loss: 0.0005426583811640739, Total Loss: 0.0005426583811640739\n",
            "Epoch: 47, Batch: 0, Loss: 0.007306371349841356, Total Loss: 0.007306371349841356\n",
            "Epoch: 48, Batch: 0, Loss: 0.0007384048076346517, Total Loss: 0.0007384048076346517\n",
            "Epoch: 49, Batch: 0, Loss: 0.004703125916421413, Total Loss: 0.004703125916421413\n",
            "Epoch: 50, Batch: 0, Loss: 0.00017090655455831438, Total Loss: 0.00017090655455831438\n",
            "Epoch: 51, Batch: 0, Loss: 0.0006892410456202924, Total Loss: 0.0006892410456202924\n",
            "Epoch: 52, Batch: 0, Loss: 0.0005305210361257195, Total Loss: 0.0005305210361257195\n",
            "Epoch: 53, Batch: 0, Loss: 0.0027486449107527733, Total Loss: 0.0027486449107527733\n",
            "Epoch: 54, Batch: 0, Loss: 0.0015916122356429696, Total Loss: 0.0015916122356429696\n",
            "Epoch: 55, Batch: 0, Loss: 0.0011472629848867655, Total Loss: 0.0011472629848867655\n",
            "Epoch: 56, Batch: 0, Loss: 0.010140655562281609, Total Loss: 0.010140655562281609\n",
            "Epoch: 57, Batch: 0, Loss: 0.0011019392404705286, Total Loss: 0.0011019392404705286\n",
            "Epoch: 58, Batch: 0, Loss: 0.002341467421501875, Total Loss: 0.002341467421501875\n",
            "Epoch: 59, Batch: 0, Loss: 0.007007718086242676, Total Loss: 0.007007718086242676\n",
            "Epoch: 60, Batch: 0, Loss: 7.138818182284012e-05, Total Loss: 7.138818182284012e-05\n",
            "Epoch: 61, Batch: 0, Loss: 0.0022414810955524445, Total Loss: 0.0022414810955524445\n",
            "Epoch: 62, Batch: 0, Loss: 9.876331023406237e-05, Total Loss: 9.876331023406237e-05\n",
            "Epoch: 63, Batch: 0, Loss: 0.0002783195814117789, Total Loss: 0.0002783195814117789\n",
            "Epoch: 64, Batch: 0, Loss: 0.0035772109404206276, Total Loss: 0.0035772109404206276\n",
            "Epoch: 65, Batch: 0, Loss: 0.00015002686996012926, Total Loss: 0.00015002686996012926\n",
            "Epoch: 66, Batch: 0, Loss: 0.0003736558719538152, Total Loss: 0.0003736558719538152\n",
            "Epoch: 67, Batch: 0, Loss: 0.0009141683112829924, Total Loss: 0.0009141683112829924\n",
            "Epoch: 68, Batch: 0, Loss: 0.00016777581186033785, Total Loss: 0.00016777581186033785\n",
            "Epoch: 69, Batch: 0, Loss: 0.00014227043720893562, Total Loss: 0.00014227043720893562\n",
            "Epoch: 70, Batch: 0, Loss: 0.00027402411797083914, Total Loss: 0.00027402411797083914\n",
            "Epoch: 71, Batch: 0, Loss: 0.0005394425825215876, Total Loss: 0.0005394425825215876\n",
            "Epoch: 72, Batch: 0, Loss: 0.0003706160350702703, Total Loss: 0.0003706160350702703\n",
            "Epoch: 73, Batch: 0, Loss: 0.0006056519923731685, Total Loss: 0.0006056519923731685\n",
            "Epoch: 74, Batch: 0, Loss: 0.019540490582585335, Total Loss: 0.019540490582585335\n",
            "Epoch: 75, Batch: 0, Loss: 0.0006970861577428877, Total Loss: 0.0006970861577428877\n",
            "Epoch: 76, Batch: 0, Loss: 0.008744790218770504, Total Loss: 0.008744790218770504\n",
            "Epoch: 77, Batch: 0, Loss: 0.00045843925909139216, Total Loss: 0.00045843925909139216\n",
            "Epoch: 78, Batch: 0, Loss: 0.0006970498943701386, Total Loss: 0.0006970498943701386\n",
            "Epoch: 79, Batch: 0, Loss: 0.0013638008385896683, Total Loss: 0.0013638008385896683\n",
            "Epoch: 80, Batch: 0, Loss: 0.0016421738546341658, Total Loss: 0.0016421738546341658\n",
            "Epoch: 81, Batch: 0, Loss: 0.0007340057054534554, Total Loss: 0.0007340057054534554\n",
            "Epoch: 82, Batch: 0, Loss: 0.00014005255070514977, Total Loss: 0.00014005255070514977\n",
            "Epoch: 83, Batch: 0, Loss: 0.0001640359841985628, Total Loss: 0.0001640359841985628\n",
            "Epoch: 84, Batch: 0, Loss: 0.00046825705794617534, Total Loss: 0.00046825705794617534\n",
            "Epoch: 85, Batch: 0, Loss: 0.00018870655912905931, Total Loss: 0.00018870655912905931\n",
            "Epoch: 86, Batch: 0, Loss: 0.00020220743317622691, Total Loss: 0.00020220743317622691\n",
            "Epoch: 87, Batch: 0, Loss: 0.001693219062872231, Total Loss: 0.001693219062872231\n",
            "Epoch: 88, Batch: 0, Loss: 0.005663195624947548, Total Loss: 0.005663195624947548\n",
            "Epoch: 89, Batch: 0, Loss: 0.009075229056179523, Total Loss: 0.009075229056179523\n",
            "Epoch: 90, Batch: 0, Loss: 0.0064184945076704025, Total Loss: 0.0064184945076704025\n",
            "Epoch: 91, Batch: 0, Loss: 0.002257399260997772, Total Loss: 0.002257399260997772\n",
            "Epoch: 92, Batch: 0, Loss: 0.0361887663602829, Total Loss: 0.0361887663602829\n",
            "Epoch: 93, Batch: 0, Loss: 0.0016969849821180105, Total Loss: 0.0016969849821180105\n",
            "Epoch: 94, Batch: 0, Loss: 0.0007609191816300154, Total Loss: 0.0007609191816300154\n",
            "Epoch: 95, Batch: 0, Loss: 0.0018447687616571784, Total Loss: 0.0018447687616571784\n",
            "Epoch: 96, Batch: 0, Loss: 0.0007709289784543216, Total Loss: 0.0007709289784543216\n",
            "Epoch: 97, Batch: 0, Loss: 0.00034975731978192925, Total Loss: 0.00034975731978192925\n",
            "Epoch: 98, Batch: 0, Loss: 0.0001039932103594765, Total Loss: 0.0001039932103594765\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 99, Batch: 0, Loss: 0.0003044720215257257, Total Loss: 0.0003044720215257257\n",
            "Epoch: 100, Batch: 0, Loss: 0.00029752569389529526, Total Loss: 0.00029752569389529526\n",
            "Epoch: 101, Batch: 0, Loss: 0.00011103472206741571, Total Loss: 0.00011103472206741571\n",
            "Epoch: 102, Batch: 0, Loss: 0.00018468529742676765, Total Loss: 0.00018468529742676765\n",
            "Epoch: 103, Batch: 0, Loss: 0.0006430173525586724, Total Loss: 0.0006430173525586724\n",
            "Epoch: 104, Batch: 0, Loss: 0.0019401067402213812, Total Loss: 0.0019401067402213812\n",
            "Epoch: 105, Batch: 0, Loss: 0.0011122183641418815, Total Loss: 0.0011122183641418815\n",
            "Epoch: 106, Batch: 0, Loss: 0.00022784658358432353, Total Loss: 0.00022784658358432353\n",
            "Epoch: 107, Batch: 0, Loss: 0.00010953752644127235, Total Loss: 0.00010953752644127235\n",
            "Epoch: 108, Batch: 0, Loss: 0.0006609426927752793, Total Loss: 0.0006609426927752793\n",
            "Epoch: 109, Batch: 0, Loss: 0.0011653536930680275, Total Loss: 0.0011653536930680275\n",
            "Epoch: 110, Batch: 0, Loss: 9.761394903762266e-05, Total Loss: 9.761394903762266e-05\n",
            "Epoch: 111, Batch: 0, Loss: 0.00019023672211915255, Total Loss: 0.00019023672211915255\n",
            "Epoch: 112, Batch: 0, Loss: 0.001032070955261588, Total Loss: 0.001032070955261588\n",
            "Epoch: 113, Batch: 0, Loss: 0.00036400972749106586, Total Loss: 0.00036400972749106586\n",
            "Epoch: 114, Batch: 0, Loss: 0.0005802473169751465, Total Loss: 0.0005802473169751465\n",
            "Epoch: 115, Batch: 0, Loss: 0.0012454009847715497, Total Loss: 0.0012454009847715497\n",
            "Epoch: 116, Batch: 0, Loss: 0.0019889026880264282, Total Loss: 0.0019889026880264282\n",
            "Epoch: 117, Batch: 0, Loss: 0.00016887644596863538, Total Loss: 0.00016887644596863538\n",
            "Epoch: 118, Batch: 0, Loss: 0.005329493433237076, Total Loss: 0.005329493433237076\n",
            "Epoch: 119, Batch: 0, Loss: 0.001989157870411873, Total Loss: 0.001989157870411873\n",
            "Epoch: 120, Batch: 0, Loss: 0.001784623833373189, Total Loss: 0.001784623833373189\n",
            "Epoch: 121, Batch: 0, Loss: 0.0014945592265576124, Total Loss: 0.0014945592265576124\n",
            "Epoch: 122, Batch: 0, Loss: 0.00039823673432692885, Total Loss: 0.00039823673432692885\n",
            "Epoch: 123, Batch: 0, Loss: 0.00015791290206834674, Total Loss: 0.00015791290206834674\n",
            "Epoch: 124, Batch: 0, Loss: 0.0012281278613954782, Total Loss: 0.0012281278613954782\n",
            "Epoch: 125, Batch: 0, Loss: 0.0021321524400264025, Total Loss: 0.0021321524400264025\n",
            "Epoch: 126, Batch: 0, Loss: 0.0008827329729683697, Total Loss: 0.0008827329729683697\n",
            "Epoch: 127, Batch: 0, Loss: 0.0004451433487702161, Total Loss: 0.0004451433487702161\n",
            "Epoch: 128, Batch: 0, Loss: 0.00012105634959880263, Total Loss: 0.00012105634959880263\n",
            "Epoch: 129, Batch: 0, Loss: 0.00032668939093127847, Total Loss: 0.00032668939093127847\n",
            "Epoch: 130, Batch: 0, Loss: 0.001694289268925786, Total Loss: 0.001694289268925786\n",
            "Epoch: 131, Batch: 0, Loss: 0.00016436439182143658, Total Loss: 0.00016436439182143658\n",
            "Epoch: 132, Batch: 0, Loss: 0.0009097024449147284, Total Loss: 0.0009097024449147284\n",
            "Epoch: 133, Batch: 0, Loss: 9.606570529285818e-05, Total Loss: 9.606570529285818e-05\n",
            "Epoch: 134, Batch: 0, Loss: 0.0005795700708404183, Total Loss: 0.0005795700708404183\n",
            "Epoch: 135, Batch: 0, Loss: 0.0017707948572933674, Total Loss: 0.0017707948572933674\n",
            "Epoch: 136, Batch: 0, Loss: 0.002614856231957674, Total Loss: 0.002614856231957674\n",
            "Epoch: 137, Batch: 0, Loss: 0.00458486145362258, Total Loss: 0.00458486145362258\n",
            "Epoch: 138, Batch: 0, Loss: 0.009287660010159016, Total Loss: 0.009287660010159016\n",
            "Epoch: 139, Batch: 0, Loss: 0.005704024340957403, Total Loss: 0.005704024340957403\n",
            "Epoch: 140, Batch: 0, Loss: 0.0011757343309000134, Total Loss: 0.0011757343309000134\n",
            "Epoch: 141, Batch: 0, Loss: 0.031504835933446884, Total Loss: 0.031504835933446884\n",
            "Epoch: 142, Batch: 0, Loss: 0.00015270241419784725, Total Loss: 0.00015270241419784725\n",
            "Epoch: 143, Batch: 0, Loss: 0.0009542442858219147, Total Loss: 0.0009542442858219147\n",
            "Epoch: 144, Batch: 0, Loss: 0.0008936378872022033, Total Loss: 0.0008936378872022033\n",
            "Epoch: 145, Batch: 0, Loss: 0.00044945519766770303, Total Loss: 0.00044945519766770303\n",
            "Epoch: 146, Batch: 0, Loss: 0.00025124309468083084, Total Loss: 0.00025124309468083084\n",
            "Epoch: 147, Batch: 0, Loss: 0.00019888021051883698, Total Loss: 0.00019888021051883698\n",
            "Epoch: 148, Batch: 0, Loss: 0.00013692548964172602, Total Loss: 0.00013692548964172602\n",
            "Epoch: 149, Batch: 0, Loss: 0.00010991419549100101, Total Loss: 0.00010991419549100101\n"
          ]
        }
      ],
      "source": [
        "# Set loss_function and Optimzer\n",
        "loss_function = torch.nn.MSELoss().to(device)    # mean-squared error\n",
        "optimizer = torch.optim.Adam(lstm_model.parameters(),\n",
        "                             lr=learning_rate,\n",
        "                             weight_decay=1e-5)\n",
        "\n",
        "# Train the model\n",
        "for epoch in progress_bar(range(num_epochs)): \n",
        "    \n",
        "    total_loss = 0\n",
        "    for batch_num, data in enumerate(dataloader):\n",
        "        \n",
        "        train_data, target_data = data\n",
        "        \n",
        "        lstm_model.train()\n",
        "\n",
        "        # Zero gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Predictions\n",
        "        predict_outputs = lstm_model(train_data.to(device)) # <= Set training to CUDA device\n",
        "\n",
        "        # Obtain the loss function\n",
        "        loss = loss_function(predict_outputs, target_data.to(device))\n",
        "\n",
        "        # backward propogation\n",
        "        loss.backward()\n",
        "\n",
        "        # gradient descent\n",
        "        optimizer.step()\n",
        "\n",
        "        # Calculate Loss\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        if batch_num % 100==0:\n",
        "            print(f\"Epoch: {epoch}, Batch: {batch_num}, Loss: {loss.item()}, Total Loss: {total_loss}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6bc24317",
      "metadata": {
        "id": "6bc24317"
      },
      "source": [
        "#### Prepare Testing Data for Forecasting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f012615e",
      "metadata": {
        "id": "f012615e",
        "outputId": "0ea9a7b5-8571-42bf-e6e0-ab1398c5e022"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date</th>\n",
              "      <th>Open</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Close</th>\n",
              "      <th>Adj Close</th>\n",
              "      <th>Volume</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2022-12-21</td>\n",
              "      <td>132.979996</td>\n",
              "      <td>136.809998</td>\n",
              "      <td>132.750000</td>\n",
              "      <td>135.449997</td>\n",
              "      <td>135.243500</td>\n",
              "      <td>85928000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2022-12-22</td>\n",
              "      <td>134.350006</td>\n",
              "      <td>134.559998</td>\n",
              "      <td>130.300003</td>\n",
              "      <td>132.229996</td>\n",
              "      <td>132.028412</td>\n",
              "      <td>77852100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2022-12-23</td>\n",
              "      <td>130.919998</td>\n",
              "      <td>132.419998</td>\n",
              "      <td>129.639999</td>\n",
              "      <td>131.860001</td>\n",
              "      <td>131.658981</td>\n",
              "      <td>63814900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2022-12-27</td>\n",
              "      <td>131.380005</td>\n",
              "      <td>131.410004</td>\n",
              "      <td>128.720001</td>\n",
              "      <td>130.029999</td>\n",
              "      <td>129.831772</td>\n",
              "      <td>69007800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2022-12-28</td>\n",
              "      <td>129.669998</td>\n",
              "      <td>131.029999</td>\n",
              "      <td>125.870003</td>\n",
              "      <td>126.040001</td>\n",
              "      <td>125.847855</td>\n",
              "      <td>85438400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>2022-12-29</td>\n",
              "      <td>127.989998</td>\n",
              "      <td>130.479996</td>\n",
              "      <td>127.730003</td>\n",
              "      <td>129.610001</td>\n",
              "      <td>129.412415</td>\n",
              "      <td>75703700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>2022-12-30</td>\n",
              "      <td>128.410004</td>\n",
              "      <td>129.949997</td>\n",
              "      <td>127.430000</td>\n",
              "      <td>129.929993</td>\n",
              "      <td>129.731918</td>\n",
              "      <td>77034200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>2023-01-03</td>\n",
              "      <td>130.279999</td>\n",
              "      <td>130.899994</td>\n",
              "      <td>124.169998</td>\n",
              "      <td>125.070000</td>\n",
              "      <td>124.879326</td>\n",
              "      <td>112117500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>2023-01-04</td>\n",
              "      <td>126.889999</td>\n",
              "      <td>128.660004</td>\n",
              "      <td>125.080002</td>\n",
              "      <td>126.360001</td>\n",
              "      <td>126.167366</td>\n",
              "      <td>89113600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>2023-01-05</td>\n",
              "      <td>127.129997</td>\n",
              "      <td>127.769997</td>\n",
              "      <td>124.760002</td>\n",
              "      <td>125.019997</td>\n",
              "      <td>124.829399</td>\n",
              "      <td>80962700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>2023-01-06</td>\n",
              "      <td>126.010002</td>\n",
              "      <td>130.289993</td>\n",
              "      <td>124.889999</td>\n",
              "      <td>129.619995</td>\n",
              "      <td>129.422394</td>\n",
              "      <td>87754700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>2023-01-09</td>\n",
              "      <td>130.470001</td>\n",
              "      <td>133.410004</td>\n",
              "      <td>129.889999</td>\n",
              "      <td>130.149994</td>\n",
              "      <td>129.951584</td>\n",
              "      <td>70790800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>2023-01-10</td>\n",
              "      <td>130.259995</td>\n",
              "      <td>131.259995</td>\n",
              "      <td>128.119995</td>\n",
              "      <td>130.729996</td>\n",
              "      <td>130.530701</td>\n",
              "      <td>63896200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>2023-01-11</td>\n",
              "      <td>131.250000</td>\n",
              "      <td>133.509995</td>\n",
              "      <td>130.460007</td>\n",
              "      <td>133.490005</td>\n",
              "      <td>133.286499</td>\n",
              "      <td>69458900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>2023-01-12</td>\n",
              "      <td>133.880005</td>\n",
              "      <td>134.259995</td>\n",
              "      <td>131.440002</td>\n",
              "      <td>133.410004</td>\n",
              "      <td>133.206619</td>\n",
              "      <td>71379600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>2023-01-13</td>\n",
              "      <td>132.029999</td>\n",
              "      <td>134.919998</td>\n",
              "      <td>131.660004</td>\n",
              "      <td>134.759995</td>\n",
              "      <td>134.554550</td>\n",
              "      <td>57809700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>2023-01-17</td>\n",
              "      <td>134.830002</td>\n",
              "      <td>137.289993</td>\n",
              "      <td>134.130005</td>\n",
              "      <td>135.940002</td>\n",
              "      <td>135.732758</td>\n",
              "      <td>63646600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>2023-01-18</td>\n",
              "      <td>136.820007</td>\n",
              "      <td>138.610001</td>\n",
              "      <td>135.029999</td>\n",
              "      <td>135.210007</td>\n",
              "      <td>135.003876</td>\n",
              "      <td>69672800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>2023-01-19</td>\n",
              "      <td>134.080002</td>\n",
              "      <td>136.250000</td>\n",
              "      <td>133.770004</td>\n",
              "      <td>135.270004</td>\n",
              "      <td>135.063782</td>\n",
              "      <td>58280400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>2023-01-20</td>\n",
              "      <td>135.279999</td>\n",
              "      <td>138.020004</td>\n",
              "      <td>134.220001</td>\n",
              "      <td>137.869995</td>\n",
              "      <td>137.659805</td>\n",
              "      <td>80223600</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          Date        Open        High         Low       Close   Adj Close   \n",
              "0   2022-12-21  132.979996  136.809998  132.750000  135.449997  135.243500  \\\n",
              "1   2022-12-22  134.350006  134.559998  130.300003  132.229996  132.028412   \n",
              "2   2022-12-23  130.919998  132.419998  129.639999  131.860001  131.658981   \n",
              "3   2022-12-27  131.380005  131.410004  128.720001  130.029999  129.831772   \n",
              "4   2022-12-28  129.669998  131.029999  125.870003  126.040001  125.847855   \n",
              "5   2022-12-29  127.989998  130.479996  127.730003  129.610001  129.412415   \n",
              "6   2022-12-30  128.410004  129.949997  127.430000  129.929993  129.731918   \n",
              "7   2023-01-03  130.279999  130.899994  124.169998  125.070000  124.879326   \n",
              "8   2023-01-04  126.889999  128.660004  125.080002  126.360001  126.167366   \n",
              "9   2023-01-05  127.129997  127.769997  124.760002  125.019997  124.829399   \n",
              "10  2023-01-06  126.010002  130.289993  124.889999  129.619995  129.422394   \n",
              "11  2023-01-09  130.470001  133.410004  129.889999  130.149994  129.951584   \n",
              "12  2023-01-10  130.259995  131.259995  128.119995  130.729996  130.530701   \n",
              "13  2023-01-11  131.250000  133.509995  130.460007  133.490005  133.286499   \n",
              "14  2023-01-12  133.880005  134.259995  131.440002  133.410004  133.206619   \n",
              "15  2023-01-13  132.029999  134.919998  131.660004  134.759995  134.554550   \n",
              "16  2023-01-17  134.830002  137.289993  134.130005  135.940002  135.732758   \n",
              "17  2023-01-18  136.820007  138.610001  135.029999  135.210007  135.003876   \n",
              "18  2023-01-19  134.080002  136.250000  133.770004  135.270004  135.063782   \n",
              "19  2023-01-20  135.279999  138.020004  134.220001  137.869995  137.659805   \n",
              "\n",
              "       Volume  \n",
              "0    85928000  \n",
              "1    77852100  \n",
              "2    63814900  \n",
              "3    69007800  \n",
              "4    85438400  \n",
              "5    75703700  \n",
              "6    77034200  \n",
              "7   112117500  \n",
              "8    89113600  \n",
              "9    80962700  \n",
              "10   87754700  \n",
              "11   70790800  \n",
              "12   63896200  \n",
              "13   69458900  \n",
              "14   71379600  \n",
              "15   57809700  \n",
              "16   63646600  \n",
              "17   69672800  \n",
              "18   58280400  \n",
              "19   80223600  "
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Display testdata for forecasting\n",
        "testdata.head(20)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "75bb9381",
      "metadata": {
        "id": "75bb9381"
      },
      "source": [
        "#### Feature Scaling Testing Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "32145092",
      "metadata": {
        "id": "32145092"
      },
      "outputs": [],
      "source": [
        "# Feature scaling\n",
        "min_max = (0,1)\n",
        "# feature = \"Open\" #<= same as before\n",
        "\n",
        "# Get the scaled Open\n",
        "scale_test_open = MinMaxScaler(feature_range = min_max)\n",
        "open_price = testdata[feature].values.reshape(-1, 1)\n",
        "data_test_open = scale_test_open.fit_transform(open_price) # Scale the Open Price\n",
        "\n",
        "# Get the dates\n",
        "data_date = testdata.Date.values"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7e0c3a85",
      "metadata": {
        "id": "7e0c3a85"
      },
      "source": [
        "#### Prepare Testing Data with Sliding Window"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f092854f",
      "metadata": {
        "id": "f092854f",
        "outputId": "489747dc-b4d1-4f1f-f875-9acca144e2df"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Index of Date: 7\n",
            "Start Date: 2023-01-03\n",
            "\n",
            "Test shape: (76, 7, 1)\n",
            "\n",
            "Test:\n",
            "batch size: 76\n",
            "sequence length: 7 days\n",
            "input size: 1\n",
            "\n",
            "\n",
            "Actual shape: (76, 1)\n",
            "\n",
            "Actual:\n",
            "batch size: 76\n",
            "input size: 1\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Extract data with sliding window\n",
        "# Sequence_length same for train dataset\n",
        "# year = \"2023\" # Same year as Training\n",
        "# month = \"01\" # Same Month as Training\n",
        "\n",
        "\n",
        "index_start_date = start_date_index(year, month, testdata)\n",
        "\n",
        "test, actual, date, open_price = sliding_window_test(index_start_date, sequence_length, \n",
        "                                                     data_test_open, open_price, data_date)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "211730e0",
      "metadata": {
        "id": "211730e0"
      },
      "source": [
        "#### Prepare Testing Data for Forecasting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9cb24dc7",
      "metadata": {
        "id": "9cb24dc7"
      },
      "outputs": [],
      "source": [
        "# Set data to Tensor\n",
        "test = Variable(torch.Tensor(np.array(test).reshape(-1,sequence_length,1)))\n",
        "actual = Variable(torch.Tensor(np.array(actual).reshape(-1,1)))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d6521043",
      "metadata": {
        "id": "d6521043"
      },
      "source": [
        "### Model Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dac6ba42",
      "metadata": {
        "id": "dac6ba42"
      },
      "outputs": [],
      "source": [
        "# Model prediction\n",
        "lstm_model.eval()\n",
        "with torch.no_grad():\n",
        "    prediction = lstm_model(test.to(device))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "14892937",
      "metadata": {
        "id": "14892937"
      },
      "source": [
        "#### Inverse Scale Prediction and Prepare for Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11640031",
      "metadata": {
        "id": "11640031",
        "outputId": "cf2d9d06-3118-4854-9eb0-7409591ec5c9"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date</th>\n",
              "      <th>Actual_Open</th>\n",
              "      <th>Predicted_Open</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2023-01-03</td>\n",
              "      <td>130.279999</td>\n",
              "      <td>129.136429</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2023-01-04</td>\n",
              "      <td>126.889999</td>\n",
              "      <td>130.991425</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2023-01-05</td>\n",
              "      <td>127.129997</td>\n",
              "      <td>128.414845</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2023-01-06</td>\n",
              "      <td>126.010002</td>\n",
              "      <td>127.984885</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2023-01-09</td>\n",
              "      <td>130.470001</td>\n",
              "      <td>126.974983</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>71</th>\n",
              "      <td>2023-04-17</td>\n",
              "      <td>165.089996</td>\n",
              "      <td>164.271827</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>72</th>\n",
              "      <td>2023-04-18</td>\n",
              "      <td>166.100006</td>\n",
              "      <td>165.214203</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>73</th>\n",
              "      <td>2023-04-19</td>\n",
              "      <td>165.800004</td>\n",
              "      <td>166.087307</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>74</th>\n",
              "      <td>2023-04-20</td>\n",
              "      <td>166.089997</td>\n",
              "      <td>165.905006</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75</th>\n",
              "      <td>2023-04-21</td>\n",
              "      <td>165.050004</td>\n",
              "      <td>165.995487</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>76 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          Date  Actual_Open  Predicted_Open\n",
              "0   2023-01-03   130.279999      129.136429\n",
              "1   2023-01-04   126.889999      130.991425\n",
              "2   2023-01-05   127.129997      128.414845\n",
              "3   2023-01-06   126.010002      127.984885\n",
              "4   2023-01-09   130.470001      126.974983\n",
              "..         ...          ...             ...\n",
              "71  2023-04-17   165.089996      164.271827\n",
              "72  2023-04-18   166.100006      165.214203\n",
              "73  2023-04-19   165.800004      166.087307\n",
              "74  2023-04-20   166.089997      165.905006\n",
              "75  2023-04-21   165.050004      165.995487\n",
              "\n",
              "[76 rows x 3 columns]"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Inverse scale transform\n",
        "final_actual = scale_test_open.inverse_transform(actual)\n",
        "final_prediction = scale_test_open.inverse_transform(prediction.cpu())\n",
        "\n",
        "# Combine Actual with Predict\n",
        "output_df = pd.DataFrame(data={\"Date\":date.reshape(-1,),\n",
        "                               \"Actual_Open\":final_actual.reshape(-1,), \n",
        "                               \"Predicted_Open\":final_prediction.reshape(-1,)})\n",
        "output_df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aac58c10",
      "metadata": {
        "id": "aac58c10"
      },
      "source": [
        "#### Calculate Model RMSE Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "70347698",
      "metadata": {
        "id": "70347698",
        "outputId": "ee79529d-344b-410e-e0ec-6479004abdfa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MSE Loss: 4.452670097351074 \n",
            "RMSE Loss: 2.110135078430176\n",
            "\n",
            "MSE Loss: 4.452668682285069 \n",
            "RMSE Loss: 2.1101347545322953\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Calculate Model RMSE Loss\n",
        "rmse_loss_np, mse_np = MSE_RMSE_loss_np(final_prediction, final_actual)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "933c15e3",
      "metadata": {
        "id": "933c15e3"
      },
      "source": [
        "#### Calculate Model R2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7fd1d08",
      "metadata": {
        "id": "c7fd1d08",
        "outputId": "97ea7551-b1c3-47a7-8d8a-8eaea967a8b4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "R2: 0.9626431433358783\n",
            "\n",
            "R2: 0.9626431433358783\n",
            "\n"
          ]
        }
      ],
      "source": [
        "r2_np = R2_np(final_prediction, final_actual)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b59c0974",
      "metadata": {
        "id": "b59c0974"
      },
      "source": [
        "### Save Model Forecast Results and Train Data Used"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e97998e",
      "metadata": {
        "id": "8e97998e"
      },
      "outputs": [],
      "source": [
        "if not os.path.exists(\"dataset\"):\n",
        "    os.mkdir(\"dataset\")\n",
        "output_df.to_csv(r\"dataset\\outputSingle.csv\")\n",
        "traindata.to_csv(r\"dataset\\traindataSingle.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cab43ac7",
      "metadata": {
        "id": "cab43ac7"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}